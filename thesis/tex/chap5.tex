\chapter{Graph Experiments}

\textbf{\#TODO: run these again}

As part of our research into more efficiently matching founder with investors, we sought to quantitatively demonstrate some of the commonly-accepted characteristics of fundraising. Furthermore, we wish to explore how we can leverage automated ranking systems to more approriately match companies with a source of funding. To do this, we ran various experiements on a social graph of founders, investors, and their mutual connections. This graph is built from the information provided by founders on the VCWiz platform.

As discussed in the previous chapter, one of the features of VCWiz is a CRM for founders, which integrates with their inbox and scans (the headers of) all their emails back and forth with investors registered on the platform. As part of this optional integration, founders gave permission to have their aggregate email data used for research. As we scan these emails (filtering out any irrelevant ones, as described in Listing \ref{code:parse}), we build up a graph, where each node represents an individual (using their email as a unique key), and each edge represents an email connection between two nodes. Edges are directed: there exists an edge from node $i$ to node $j$ if and only if an email has been sent from email $e_i$ to email $e_j$. Edges have weights equal to the total such number of emails sent. These weights are ignored when doing manipulations and calculating graph metrics, unless explicitly specified.

Our motivation for modeling the underlying graph for these experiments is as follows. The majority of pre-pitch and post-pitch communication when fundraising happens over email, and almost every introduction made to an investor on behalf of a founder is done by email as well. Furthermore, emails are often sent as follow-ups to in-person meetings (at networking events, etc.). Finally, email is the preferred medium for ongoing communication between and founder and their investors. Thus, by capturing the entirety of the email graph for the subset of founders and investors on our platform, we get an accurate picture of what relationships are at play.

\section{Experiments}

We first summarize the experiments run, and the high-level results of these experiements. The remainder of this chapter will dive into the details of each one. The goal of these experiments is to better identify what correlates with a succesful fundraise, and how we can use this information to better match founders and investors, for the purpose of maximizing funding likelihood.

\textbf{\#TODO: summarize experiments and findings}

- Founder Graph Analysis
- Predicting Fundraising Success with Graph Metrics

It has long been supposed that the characteristics of a founder in his or her professional network can impact, and indeed predict, how successful a fundraising attempt will be. For the purposes of these experiments, we will define success as raising at least as much money as planned, from a founder's top pick of investors, in as short a time as possible. There is lots of annecdotal evidence to support these claims (\textbf{TODO cite a blog post or two}), and recently there has been statistical evidence as well. A 2017 study uses \textbf{TODO what is the data source?} data ``to estimate the effects of network distance in the matches resulting from Series A financing rounds'', and concludes that ``distance drives matching value and moderates preferences for experience and education''~\cite{pasquini2017matching}. We sought to verify and further elucidate this point with our graph data from VCWiz.

\section{Preprocessing}

Before we could analyze the graph, we had to do some preprocessing. The first step was of course to build the graph. We followed the steps in Section \ref{vcwiz:ingesting} (p. \pageref{vcwiz:ingesting}) to import each founder's emails, adding nodes and edges to a global graph as described above. Any messages skipped in the processing phase do not have their addresses added to the graph. We added several additional rules for messages to skip based on analyzing intermediate graphs for outliers (for example, nodes which had significantly higher than average in-degrees or out-degrees).

Once the graph is set up, there is one last preprocessing step that must occur. Often, there are founders who sign up for the platform, but never interact with any of the email-related features. Their nodes still get added to the graph, but are orphans that have no neighbors. These nodes can slow down metric calculations uncessarily, and make analysis harder, so we first filter them out using the Cyper query in Listing \ref{vcwiz:cypher:orphans}.

The final step is to label each node in the graph. Every node is labeled as a \texttt{Person}, with known investors and founders being labeled as \texttt{Investor} and \texttt{Founder} respectively. These two labels are mutually exclusive. In the case a node could be labeled as both an \texttt{Investor} and \texttt{Founder}, it is treated as a investor if the person is currently employed by an institutional investment firm, and a founder otherwise.

\section{Founder Graph Analysis}

Before diving into our main experiments, we did some analysis of the basic graph that was built up in the last section. At the time of writing, the VCWiz email graph has 216,774 individual nodes, with 726 verified founders and 789 verified investors.

\subsection{Connectivity}

Looking at just the subgraph of verified founders, we see that each node has a mean of $5.5$ neighbors, indicating that the founders on the platform often know other founder on the platform. This is consistent with the real-world behaviour of early-stage founders, who often communicate with a clique of other similar-stage founders and share resources such as tools. Indeed, it is possible that this connectivity is the result of founders spreading the word about the tool to their peers. An interesting observation here is that though these founders are not as professionally isolated as those who would benefit most from using the tool, they are perfectly set up to use the network-based functionality of VCWiz, such as Intro Paths.

\subsection{Communities}

In order to determine how much of the connectivity present is the result of founders sharing the tool with peers, we performed a connectivity analysis. If it turns out that the communities of the graph are isolated globally but strongly connected locally, it would support our hypothesis. Using Label Propagation (LPA)~\cite{2007PhRvE..76c6106R}, we can section the graph into partitions by flooding the nodes with labels: assigning each node an initial label and propagating these labels with a set of rules until distinct communities evolve.

Upon paritioning the founder graph, we find that there are 584 communities, with an average of 1.2 founders per community. Our model of the founder community on the platform was not accurate.

An interesting finding is that the few most-populous communities are easily recognizable, after which there is a long tail of independent communities with only one or two founders. The three top communities found by LPA are Dorm Room Fund Partners, Dorm Room Fund Portfolio Companies, and YCombinator Portfolio Companies. Given that both of these organizations helped influence this work, it is not surprising to find these communities.

We also ran an alternative connectivity analysis, using the Louvain Method (\textbf{CITE}). This revealed another significant community of founders: Student Founders at UC Berkeley. However, the remainder of the communities still appear to be insignificant.

\subsection{Propensity to Investors}

We sought to answer the question of whether or not the community and neighborhood of a founder's node can predict their propensity to engage with certain investors. However, using graph structure alone, we lack sufficient signal to predict anything. We will later revisit this question, taking into account additional node metrics and founder characteristics.

\section{Predicting Fundraising Success with Graph Metrics}

We would like to explore whether or not linear combinations of simple graph metrics can predict fundraising success. We first will define our metrics, and their intuitive meaning within the context of fundraising.  We hypothesize that the metrics which correspond to commonly-accepted key factors to a foudner's ability to fundraise in venture capital will be highly correlated with our definition of success. We will attempt to validate this hypothesis with our email graph, as well as analyze which factors are indeed the most important.

\subsection{Baseline Ranking}

In order to evaluate our scoring function based on simple graph metrics, we need a baseline to compare against. In the absense of a quantitative baseline scoring function, we hand-crafted a baseline ranking, incorporating our knowledge of venture capital, and the results of the research cited thus far. We will breifly document the process of exploring features for this baseline before defining the actual scoring function.

Below are the features that were considered for the function, and our evaluation of each one for inclusion.

\subsubsection{Total funding of current round}

When available, the total amout of money raised in the current round is an excellend indicator of fundraising success, as it is normally the goal of the founder to raise as much money as possible, up to some internal maximum. This metric should definetly be included in the scoring function. Unfourtunately, for many founders on the platform who are currently attempting to raise their seed rounds of funding, there might not yet be any money raised.

\subsubsection{Total funding of previous rounds}

As referenced earlier in Section \ref{chap3:tool} (p. \pageref{chap3:tool}), the First Round Capital 10 Year Project \cite{first-round-10-years} indicated much higher fundraising success rates for repeat founders. In this case, having raised money either in a previous round for the same company, or for a previous company, would give a founder the credibility and experience necessary to improve their chances for their current fundraise. We experimented including both this number, when available, as well as a binary feature indicating that this number is nonzero. Ultimately, we decided to use the raw number, as variations in this metric are significant.

\subsubsection{Number of intro requests accepted}

While it would be useful to include a metric capturing a founder's success rate when requesting cold introductions on the VCWiz platform, there is simply not enough information for the metric we report to be indicative of anything. This follows from our earlier analysis (Section \ref{chap4:introrequests}) on why this feature saw very little usage.

\subsubsection{Number of interested investors}

A metric which is indicative of global investor interest in a company is the number of investors who have exchanged multiple emails with the founder during the timeframe of the raise. While these investors may or may not end up investing, the fact that they were interested enough to email several times is a strong signal that the founder will have options to select from when it comes time to close the round.

\subsubsection{Fraction of investors who respond}

A similar metric to the last is the percentage of investors who have emailed a founder back after the founder has initiated contact with them (either directly or through an introduction). A high value here indicates that the founder and their company are compelling enough to garner investor interest, and will have several options to select from when fundraising is over. It's also indiative of a founder's ability to reach out to investors who are a good fit for the startup.

\subsubsection{Average response time per investor}

The average time an investor takes to respond to a founder's email would be indicative of the investor's excitment for the founder, if all such communication happened over email and was captured by the platform. However, many founders are not using the email integrations of VCWiz, and many more will have the most crucial conversations by phone or in-person. This can create outliers which heavily skew the average and add a burdensome level of noise. Thus, this feature will not be included.

\subsubsection{Length of fundraising period}

A strong fundraise (based on our earlier criteria) is one which results in sufficient dollars being raised, in the shortest amount of time possible. Thus, the amount of time spent fundraising should be a high-signal feature. However, we observed a pattern of utilization which makes this feature problematic. As founders reach the stage of their fundraising where they are pitching venture firms and finalizing the details of the deal, they often stop using the VCWiz platform, as they are now working with a sufficiently small number of investors. As a result, it is very difficult for us to estimate when a fundraise is over, and we do not have accurate data for this feature.

\textbf{TODO: plot, averaged over all founders who raised, of email volume vs percentage of target investors who have decided one way or another, to show that email volume decreases as most investors have decided}

\subsubsection{Average sentiment of investors}

Intuitively, a high positive sentiment from an investor in an email is one feature of many that indicates an affinity, and a strong relationship. Looking at the data on VCWiz, strong sentiment is often associated with a pre-existing relationship, often from a previous company. Thus, while it is a noisy source of data, it should be included in our ranking function.

\textbf{TODO: plot, across all founders who raised on the platform, average sentiment of incoming emails vs total money raised}

\textbf{TODO: plot, across all founders who raised on the platform, average sentiment of incoming emails vs number of interested featured investors}

\textbf{TODO: consider normalizing against average sentiment of each investor globally}

\medskip

\subsubsection{Evaluation}

After evaluating each of the above metrics, we drew the following conclusions. The best metric, when available, is the aggregate funding the founder has raised to date, across any company. This is the sum of the two funding-related features above. Following this, average sentiment is a good proxy for founders who have raised no money but are actively engaged in conversations. The number of interested investors is also a good proxy during the fundraise, as it represents absolute interest. The email outreach success rate (fraction of investors who respond) tends to be too noisy, as it is skewed by high-profile founders who send a very small number of emails, to already-established connections. However, the number of investors who respond after being added to a wishlist on VCWiz is a very strong signal, as it indicates no prior relationship.

\subsubsection{Ranking Function}

To create a ranking function from these features, we calculate a weighted sum of sort indexes for each founder, and use the position of this score in the list of all scores as the rank. We use ranks, not absolute values, across features to account for differences in scale.

We demonstrate the scoring function in Equation \ref{eq:baseline}, where $S_{m, f}$ gives the sort index for founder $f$ when the founders are sorted by metric $m$, and $W$ is the set of (metric, weight) pairs. The final ordering of the baseline is the set of fouders, sorted by this score, in ascending order (a lower score represents a higher-ranked founder).

\begin{equation}
\label{eq:baseline}
  score(f) := \sum_{(m, w) \in F} S_{m, f} * w
\end{equation}

\noindent The weights we use are found in Figure \ref{fig:baseline:weights}.

\begin{figure}[ht]
\begin{tabular}{c | c}
\textbf{Feature}           & \textbf{Weight} \\\hline
Aggregate Funding          & 4 \\\hline
Average Inbound Sentiment  & 3 \\\hline
\# Responded From Waitlist & 2 \\\hline
\# Interested              & 1.5
\end{tabular}
\centering
\caption{Baseline Weights}
\label{fig:baseline:weights}
\end{figure}

This baseline, while not defensible as a ground-truth ranking for rigorous statistics, is built on sane assumptions about fundraising and venture capital. Random sampling indicates that the results are aligned with expert expectations. We will use this baseline to compare and evaluate our numerical methods for ranking, but acknowledge that it is not a perfect ranking according to the criteria we defined.

\subsection{Evaluation}

The next step is to define how to evaluate other ranking functions against our baseline. There are many options for evaluating ranking functions. We will discuss the options before justifying our selection. For mathematical definitions of each evaluation metric independent of our use case, we refer the reader to Section 3.2 of \cite{DBLP:journals/corr/abs-0704-3359}. These ranking functions are often used to score a permutation $\pi$ of documents given a document-set, query tuple $(D, q)$. In this case, we assume $q$ is the fixed query of founders which are most likely to succeed at fundraising, and $D$ is our list of founders.

In selecting the evaluation criteria, we imagine two relevant use-cases for our ranking functions. The first is to display a sorted list of founders to an interested party, such as an investor. The second is to use this ranking as a feature in a later process, such as enhanced matching between founders and investors (which we will explore in the next chapter).

Winner Takes All (WTA) and Mean Reciprocal Rank (MRR) are often used when displaying results based on a ranking, but both assume there is only \textit{one} top-ranked document. This does not align with either of our uses cases, so we discard them.

Discounted Cumulative Gain (DCG) takes the sum of \textit{relevances} of each founder in the ranked list (where the relevance in our case is $N - r$, with $N$ being the total number of founders, and $r$ being the rank of that founder in the baseline), weighting each founder by how early it appears in the list. We therefore get a score which increases as we put the highest-ranked founders near the start of the list, but penalizes incorrect ordering of later founders less and less. In other words, this is a metric of relative regret. This aligns well with our first use case, and provides some useful, though imperfect, information for our second.

Normalized Discounted Cumulative Gain (NDCG) simply normalizes the DCG against the number of founders in the list, so the metric can be compared across lists of different size.

Precision@n is another metric which gives preference to the top results, in this case explicitly only considering the top $n$. This metric is simply the fraction of founders in the top $n$ results that are also in the top $n$ founders of the baseline. It is a quick, useful way of evaluating a ranking for our first use case.

Root-Mean-Square Error (RMSE) is a very common error function for recommender systems (\cite{Cremonesi:2010:PRA:1864708.1864721})) which measures the differences (errors) in ranking for every founder in the list. It works very well for our second use case.

Mean Absolute Error (MAE) is another common error function, which measures the average absolute distance between a ranking and the ranking of the baseline.

Kendall's rank correlation coefficient ($\tau$) measures the ordinal association between the two lists of founders, and is often used to report on the correlation between to rankings. It roughly measures the agreement in rank over all pairs of items.

Spearman's rank correlation coefficient ($\rho$) is another measure of rank correlation. It measures the direction of association in rank between the two lists of founders.

We decided to use NDCG, Precision@n (for various values of $n$) for evaluating the quality of the rankings for the purposes of recommendation, $\tau$ and $\rho$ for calculating the rank correlation with the baseline, and RMSE and MAE for scoring the ranking as a feature to later processes. Figure \ref{fig:evaluation:formulas} shows the formulas we use for each, where $N$ is the number of founders in the list, $B$ is the baseline, $X$ is the ranking being evaluated, $R_i$ is the founder in the $i$-th position of ranking $R$, $\text{rg}(f)$ gives the rank of founder $f$ in the baseline, and $\text{sc}(R, f)$ gives the \textit{score} of founder $f$ under ranking $R$. Each ranking that we will be evaluatng has a scoring function, which is in the range $[0, 1]$ for each founder. For the baseline, we assign scores by simply scaling the rank to be in this range.

\begin{figure}[ht]
\begin{equation}
  \begin{array}{c >{$\displaystyle}Sc<{$}}
    \text{\textbf{Metric}} & \text{\textbf{Forumla}} \\
    \text{DCG}(X) & \sum_{i=1}^{N} \frac{2^{N - \text{rg}(X_i)} - 1}{\log_2{i + 1}} \\
    \text{NDCG}(X) & \frac{\text{DCG}(X)}{\text{DCG}(B)} \\
    \text{Precision}(X, n) & \frac{|X_{0:n} \bigcap B_{0:n}|}{n} \\
    \tau(X) & \frac{\sum_{i=1,j=1,j \neq i}^N \text{conc}(i, j, X)}{N (N - 1)} \\
    \rho(X) & 1 - \frac{6 \cdot \sum_{i=1}^N \text{drg}^2(X, B, i, i)}{N (N^2 - 1)} \\
    \text{RMSE}(X) & \sqrt{\frac{\sum_{i=1}^N \big(\text{dsc}(X, B, X_i)\big)^2}{N}} \\
    \text{MAE}(X) & \frac{\sum_{i=1}^N \big|\text{dsc}(X, B, X_i)\big|}{N}
  \end{array}
\end{equation}

\begin{align*}
 \text{drg}(X, Y, i, j) &:= \text{rg}(X_i) - \text{rg}(Y_j) \\
 \text{dsc}(X, Y, f) &:= \text{sc}(X, f) - \text{sc}(Y, f) \\
 \text{conc}(X, i, j) &:= \text{sign}\big(\text{drg}(X, X, i, j)\big) \cdot \text{sign}\big(\text{drg}(B, B, i, j)\big)
\end{align*}

\centering
\caption{Evaluation Criteria}
\label{fig:evaluation:formulas}
\end{figure}

Finally, we must define our null hypothesis, so that we can provide $p$-values for our rank correlations. In this case, the null hypothesis is that the correlation between the baseline and the ranking in question is 0. Thus, $p$ gives the probability that the two uncorrelated rankings would give them same rank correlation metric value.

--

Get these for the baseline. Use rank scaled to 0-1 as the relevance.
Then, treat it as a linear regression problem, with the sole goal of learning the weights on the three metrics, to see which is most important. Write up conclusions from that, compare RMSE of naive and weighted.
Then, see if we can use these three features in other ways, to try to get the best ranking possible, given the labels from the baseline. Use a couple of the fancy methods from \footnote{\url{https://en.wikipedia.org/wiki/Learning_to_rank}}.

\subsection{FounderRank}

We now shift our focus to ranking founders maed on the information we collected on the VCWiz platform. We wish to score nodes based on graph metrics in an email graph of fundraising relationships. To do this, we first need to define a single scoring function which captures our goals. Thus we introduce FounderRank.

FounderRank is a metric in the range $[0, 1]$ that quickly communicates the strength of a founder in the global fundraising graph of founders, investors, and their mutual connections (i.e. the VCWiz Email Graph). A strong node is able to rapidly spread the word about their startup, start conversations with relevant and desirable investors, and convince investors to invest in them. These abilities are cruical for fundraising, which is in turn crucial for the survival of a startup.

Note that we are currently evaluating how well a founder can fundraise conditioned on them knowing who they would like to fundraise from. We are not tackling the issue of discovering investors, which we have touched on in the previous chapter, and will explore further in the next.

\subsubsection{Core Characteristics}

A combination of existing studies and our own interviews with numerous seed-stage firms reveals three intuitive proprties of a founder's node in a professional or social graph which are desirable when fundraising: \textbf{importance}, \textbf{influence}, and \textbf{access}. Note that these characteristics do not take into account factors such as domain expertise, personality, and pedigree, all of which will also contribute to a successful fundraise. We are focusing exclusively on network properties for this experiment.

The first characteristic is \textbf{importance}. Importance looks at how crucial a founder is in his or her own ecosystem. In efficient professional information networks, we see that ``needs are diagnosed and [the entrepreneur] is passed round the system until [he or she] gathers the necessary information and advice.''~\cite{BIRLEY1985107}. The more crucial a founder is to a domain, the more access to information he or she will have. Thus, founders who have more importance are likely to be seen as a less risky investment, increasing the chances an investor responds positively to a fundraising proposal.

The second characteristic is \textbf{influence}. Influence captures how effectively a founder can effect change in their ecosystem and solicit aid from their peers. In other words, how likely are other founders to help this founder? A study on interorganizational networks of young companies found supporting evidence to the fact that ``third parties rely on the prominence of the affiliates of those companies to make judgments about their quality''~\cite{10.2307/2666998}. Founders who have high influence can leverage this to convince investors to give them funds.

The third characteristic is \textbf{access}. This is the notion of how well a founder can get in front of the investors of their choice. The more directly connected a founder is to an abritrary investor, the more likely that investor is to respond to an inbound request (either via an introduction or cold) for funding. Additionally, a high degree of access means a founder has many options to chose from when it comes to starting conversations with investors. This follows from the more general finding that proximity in a graph to providers of valuable resources gives a node access to many viable alternatives\cite{10.2307/3069443}. This can be valuable when a founder's top choice of investor does not work out, which is often the case.

\subsubsection{Graph Metrics}

To quantitatively evaluate the impact of each of these characteristics, and measure their predictive capability, we need to find graph metrics which correspond to each.

For \textbf{importance}, we selected PageRank~\cite{page1999pagerank}, the canonical starting point for ranking nodes in graphs. PageRank recursively evaluates node importance by analyzing the importance of nodes which link to the node in question. It is a widely-accepted measure of node imporance. We use a normalized PageRank~\cite{berberich2007comparing}, which accounts for the number of nodes and structure of the graph.

For \textbf{influence}, we selected Betweenness Centrality~\cite{10.2307/3033543}. Betweenness Centrality is the count of the number of shortest paths (over all pairs of nodes) that pass through the node in question. The rationale is that if a node is on the intro path for a pair of people, that node has influence over that pair, as it can control whether or not the introduction is made. This makes the assumption that every communication request goes along the shortest path, which is not perfectly accurate, but is sufficient for our purposes.

For \textbf{access}, we selected Closeness Centrality~\cite{FREEMAN1978215}. The Closeness Centrality of a node is inversely proportional to the node's distance from every other node. Thus, this metric measures how ``close'' a node is to the other nodes in the graph, based on shortest-path lengths, normalized for the number of nodes in the graph. The rationale is that if a node can access every other node in the graph, on average, via a short path, the node must have better access than a node that must use longer paths. Once again, this assumes that the length of the shortest path between nodes is the determiner of connection strength. Based on the data presented and cited thus fair, this is a fair assumption.

To use these metrics, we take the pre-processed graph above, and calculate the raw metric value for each node. We then normalize it as specified above, and finally scale it to be in the range $[0, 1]$. We now have a number which represents the metric for a node, relative to the other nodes in the graph, and on a standard scale. Note that we do not yet specify how to combine these metrics, we are simply calculating them.

\subsubsection{Naive FounderRank}

We now begin to explore ways to combine these three selected graph metric to produce a scoring function that we can compare against our baseline. The Naive FounderRank (NFR) method simply averages these three numbers, per node, to arrive at a score in $[0, 1]$.

The first observation we made when trying to rank the founders based on NFR is that our rankings totally missed founders on the platform who are not currently fundraising, but who are known to be repeat founders who have successfully fundraised in the past. THis was a result of our email graph only spanning the last year. Relationships in the venture capital world take years to build up, and we were not evaluating them over a large enough timeframe. Thus, we re-built our graph to incorporate the last five years of data, which gave us a much more comprehensive view of the ecosystem.

The below observations and conclusions are all drawn from this new graph, with up to five years of email data from 558 founders.



--



Order: baseline, naive, weighted, fancy.
Get [m1, m2, m3, 0-1 output] to use in python for these.

Our naive model has a NDCG of 0.29, and Precision@n of 0, 0.1, and 0.2 for n = 5, 10, and 20 respectively. While these beat out the random model (which is what analysts at VCs are effectively using currently), they don't show a strong case for relying on the absolute rankings generated from the naive model, if the goal is parity. However, if we evaluate our naive model as a feature with the goal of directionally similar rankings to the baseline, we see that the values for tau and rho (0.48 and 0.67 respectively) are more promising. For tau, we see there is reasonable agreement between the rankings, where -1 is rankings that are perfectly opposite, 0 is independent rankings, and 1 is identical rankings. We also see in rho that the direction of association in the rankings generated by the two is positive, and significantly so. This means, more often than not, the ranking of the naive model tends to increase when the baseline does.

This is promising, and it only captures a short timeline of graph interactions, which as we disucssed above, can limit the signal we get for strong founders. Perhaps by increasing the time period over which we are considering emails, we can give stronger graph signal to the top founders, and increase the DCG and Precision@n.

For numbers, look at commit 4489986b.

\subsection{Naive FounderRank (5 years back)}

Let's go back farther and see what happens.

Now up to 558 founders.

Before: NDCG was 0.29.
After: 0.45. Random also up to 0.27, showing that the results after 5 years are more random (expected, as there is more noise)

Precision@n After: Up to 0.2, 0.3, 0.3 for n = 5, 10, 20. Makes sense, as are capturing more of the social graph of repeat founders (who rank higher in the baseline), perhaps from previous raises.

Tau After: Up negligibly. Expected, as this should only influence the top founders, leaving the rest untouched.
Rho: same deal

RMSE: up a notrivial bit to 0.082 from 0.070. expected, because overall more noisy, more random using graph metrics now.
MAE: same deal

\subsection{Naive FounderRank + Frequency}

- Not viable to do in neo4j for now, unclear how high-signal it is.
- Could play around with only considering edges that have at least a certain weight

\subsection{Weighted FounderRank}

If we use the optimal linear combination of all three, we get an $r^2$ of 0.39. Our NDCG is at 0.449, up from 0.448. No change in p@n. Our tau goes to 0.490, and our rho to 0.679. These are small/negligible changes.

Relative to all three:

If we only use pagerank, we get everything worse, except a much higher NDCG (0.527), implying that if we are just worried about surfacing the best founders, they often have high pagerank, and just using pagerank is a good indicator. We get a negative $r^2$, which makes sense given the worse metrics on everything.

If we only use betweenness, everything is worse. We get a negative $r^2$, which makes sense given the worse metrics on everything.

If we only use closeness, we get a lower NDCG (0.375) but a higher P@5 (0.4), but lower P@10 (0.2), with same other P@n. Our tau goes up to 0.498 (marginally) and our RMSE goes up to 0.135. This shows that looking at only closeness (access), we can better predict the ranking of all founders, though we get worse at predicting the exact scores, and we also get worse at surfacing the best founders first. Seems to be the case that access matters more than other metrics in most cases, but a large pagerank or closeness can compensate for this. We get a positive $r^2$ (0.348), which shows that just using closeness goes give us a model that can explain about 35\% the variance of the baseline scores.

PR and closeness: $r^2$ is 0.3916, very close to the optimal weighting of all three. Coefficients are almost the same, with PR being slightly higher (0.423 and 0.389). NDCG actuallty goes up from all three to 0.533, the best out of all of these. Rho and Tau are basically the same.

Since $r^2$ doesn't change much by adding betweenness, it doesn't explain any additional variance, and its highly correlated with the combo of the other two. This is because it is already so highly correlated with PR (0.913), and somewhat correlated to closeness (0.512).

Takeaway: goal is to predict ability to raise based on just graph metrics. emphasis on getting the ``best'' founders right, as lower-ranking founders are more noisy. thus, we evaluate based on Spearman's rho and NDCG. using only closeness (access) gets us almost all the way to the best we can do (rho=0.690, ndcg=0.375), except that is misses some of the top founders that we should be more certain about. Adding in PageRank (importance) solves this, as it is the best at capturing the founders near the top (rho=0.574, ndcg=0.527). Some of the ``best'' founders have such a high PR (such high importance) that it compensates for a lower relative closeness (access). Adding betweenness (influence) to this set of features doesn't change much, because it is highly correlated to PR (importance) (r=0.913), and somewhat correlated to closeness (access) (r=0.512). Ultimately we achieve a Spearman rank correlation (comparing the ranking from just graph metrics to the ``ground-truth'' ranking) of 0.677 with a p-value of 7.28e-77 (effectively 0), and a Normalized Discounted Cumulative Gain of 0.533. This model has an $r^2$ of 0.391, and most of that comes from closeness (adding PR is only 12.8\% increase in $r^2$ == ability to explain variance). Thus we can see that influence is captured by importance, which has an impact on ability to raise, but really it is access which dictates this component of a founder's success.

This conclusion further motivates the work we've done on the VCWiz platform, where one stated goal was to increase access to investors for founders. This conclusion implies that increasing such access would be high-impact, even for those less-well-known founders who have less influence and importance in the ecosystem.

\subsection{FounderRank + Investments}

\subsection{Metrics from 2017arXiv170604229H}

Use sum of (Job IPO, Job Acquired, Executive IPO, Executive Acquired, Advisory IPO, and Advisory Acquired). Why? Table 1 \cite{2017arXiv170604229H}. ``We see that the top non-sector features are related to the past experience of the leadership (executive acquisition, executive IPO, advisory IPO, leadership age). The investor feature maximum acquisition fraction is also one of the top non-sector features. This suggests that companies with experienced and successful leadership and investors have increased drift which results in a higher exit probability.''


For founders on the platform, edges implied by a prior investment turn up most of the time in the email logs. For founders off the platform however, we have no access to their email graph. If we use amount raised, metrics from \cite{2017arXiv170604229H} (sum of Job IPO, Job Acquired, Executive IPO, Executive Acquired, Advisory IPO, and Advisory Acquired) as a crude estimate for success, and the same 3 graph metrics, but in the funding graph, as our ranking features, do we see similar results?

Notes:

- Building up ranking graph, we have to add links in both directions, VC to founder and founder to VC, otherwise no PR ever goes to the investors
- We decide to build up the whole social graph of startups. Invested, cofounded, and co-invested. Use this as the second data set to compare to.
- Dataset size: 288,720 founders. 92,843 investors. 57,131 investments.
- Just sorting by number of exits, we see a lot of founders who are now investors. This makes sense, founders who take a company from start to ``finish'' are often what venture firms are looking for in terms of experience.
- Settled on the following baseline, which takes into account that founders who have raised large amounts of capital are more trusted by VCs to take on more money in the future, but that this not nearly as important as the number of times a founder has either lead a company to a successful exit, or participated in such a company (either as an employee or an advisor)

- Affiliated Exits: 4
- Aggregate Funding: 1

Build an investment graph, where nodes are founders and investors, and there's an edge between an investor and founder if the investor funded that founder (individual investors, not firms). See the earlier section for how we inferred which partners at which firms made which investments.

Now same deal. Calculate metrics for whole graph, track it for founders. Look at the same metrics for the naive average, and the weighted version.

1. Sort by baseline metrics
  - Done, with the whole graph. Still need to do only funding graph with backlinks.
2. Run baseline, random
  - Done, looks the same.
3. Run on whole ecosystem graph: naive and weighted
  - Done, gives much less correlation (0.22 tau, 0.33 rho), and this is when we use only closeness. Once again, even more so, looks like closeness is the real important thing.
  - Look at correlation between things. Why is this result much less good? Graph is not representative? Something else going on? Have to sit and think, and also do it on graph from 4.
4. Run on just funded graph: naive and weighted


Now add OUR founders to the platform. We don't have all this historic success data about them, but if we add their email links ot the funding graph, then run everything and rank them, does it align with our original rankings? Can we use this as a proxy with new founders? Simply add their known connections to the graph, run this, and see how they rank?

\section{InvestorRank}

Given that the FounderRank metric seems indicative of a founder's ability to raise, and that raising is a crucial part of keeping a company alive, does ranking investors by the mean founder rank of the founders they've invested in give us a reasonable ranking of investors? Manually inspect it, also compare it to baseline, which can use features like \# of exists, total follow-on raised, and total valuation at IPO sum.

\textbf{TODO}: sort by baseline metrics.

Also see if it makes sense to run these graph metrics for investors on the graph themselves. See is there is correlation between InvestorRank and FounderRank of the founders invested in by that investor. How to do correlation calculation for many to many -- Pairwise?
