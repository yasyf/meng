\chapter{Graph Experiments}

\textbf{\#TODO: run these again}

As part of our research into more efficiently matching founder with investors, we sought to quantitatively demonstrate some of the commonly-accepted characteristics of fundraising. Furthermore, we wish to explore how we can leverage automated ranking systems to more approriately match companies with a source of funding. To do this, we ran various experiements on a social graph of founders, investors, and their mutual connections. This graph is built from the information provided by founders on the VCWiz platform.

As discussed in the previous chapter, one of the features of VCWiz is a CRM for founders, which integrates with their inbox and scans (the headers of) all their emails back and forth with investors registered on the platform. As part of this optional integration, founders gave permission to have their aggregate email data used for research. As we scan these emails (filtering out any irrelevant ones, as described in Listing \ref{code:parse}), we build up a graph, where each node represents an individual (using their email as a unique key), and each edge represents an email connection between two nodes. Edges are directed: there exists an edge from node $i$ to node $j$ if and only if an email has been sent from email $e_i$ to email $e_j$. Edges have weights equal to the total such number of emails sent. These weights are ignored when doing manipulations and calculating graph metrics, unless explicitly specified.

Our motivation for modeling the underlying graph for these experiments is as follows. The majority of pre-pitch and post-pitch communication when fundraising happens over email, and almost every introduction made to an investor on behalf of a founder is done by email as well. Furthermore, emails are often sent as follow-ups to in-person meetings (at networking events, etc.). Finally, email is the preferred medium for ongoing communication between and founder and their investors. Thus, by capturing the entirety of the email graph for the subset of founders and investors on our platform, we get an accurate picture of what relationships are at play.

\section{Experiments}

We first summarize the experiments run, and the high-level results of these experiements. The remainder of this chapter will dive into the details of each one. The goal of these experiments is to better identify what correlates with a succesful fundraise, and how we can use this information to better match founders and investors, for the purpose of maximizing funding likelihood.

\textbf{\#TODO: summarize experiments and findings}

- Founder Graph Analysis
- Predicting Fundraising Success with Graph Metrics

It has long been supposed that the characteristics of a founder in his or her professional network can impact, and indeed predict, how successful a fundraising attempt will be. For the purposes of these experiments, we will define success as raising at least as much money as planned, from a founder's top pick of investors, in as short a time as possible. There is lots of annecdotal evidence to support these claims (\textbf{TODO cite a blog post or two}), and recently there has been statistical evidence as well. A 2017 study uses \textbf{TODO what is the data source?} data ``to estimate the effects of network distance in the matches resulting from Series A financing rounds'', and concludes that ``distance drives matching value and moderates preferences for experience and education''~\cite{pasquini2017matching}. We sought to verify and further elucidate this point with our graph data from VCWiz.

\section{Preprocessing}

Before we could analyze the graph, we had to do some preprocessing. The first step was of course to build the graph. We followed the steps in Section \ref{vcwiz:ingesting} (p. \pageref{vcwiz:ingesting}) to import each founder's emails, adding nodes and edges to a global graph as described above. Any messages skipped in the processing phase do not have their addresses added to the graph. We added several additional rules for messages to skip based on analyzing intermediate graphs for outliers (for example, nodes which had significantly higher than average in-degrees or out-degrees).

Once the graph is set up, there is one last preprocessing step that must occur. Often, there are founders who sign up for the platform, but never interact with any of the email-related features. Their nodes still get added to the graph, but are orphans that have no neighbors. These nodes can slow down metric calculations uncessarily, and make analysis harder, so we first filter them out using the Cyper query in Listing \ref{vcwiz:cypher:orphans}.

The final step is to label each node in the graph. Every node is labeled as a \texttt{Person}, with known investors and founders being labeled as \texttt{Investor} and \texttt{Founder} respectively. These two labels are mutually exclusive. In the case a node could be labeled as both an \texttt{Investor} and \texttt{Founder}, it is treated as a investor if the person is currently employed by an institutional investment firm, and a founder otherwise.

\section{Founder Graph Analysis}

Before diving into our main experiments, we did some analysis of the basic graph that was built up in the last section. At the time of writing, the VCWiz email graph has 216,774 individual nodes, with 726 verified founders and 789 verified investors.

\subsection{Connectivity}

Looking at just the subgraph of verified founders, we see that each node has a mean of $5.5$ neighbors, indicating that the founders on the platform often know other founder on the platform. This is consistent with the real-world behaviour of early-stage founders, who often communicate with a clique of other similar-stage founders and share resources such as tools. Indeed, it is possible that this connectivity is the result of founders spreading the word about the tool to their peers. An interesting observation here is that though these founders are not as professionally isolated as those who would benefit most from using the tool, they are perfectly set up to use the network-based functionality of VCWiz, such as Intro Paths.

\subsection{Communities}

In order to determine how much of the connectivity present is the result of founders sharing the tool with peers, we performed a connectivity analysis. If it turns out that the communities of the graph are isolated globally but strongly connected locally, it would support our hypothesis. Using LPA \textbf{TODO: acronym} \cite{2007PhRvE..76c6106R}, we can section the graph into partitions by \textbf{TODO: summarize method}.

Upon paritioning the founder graph, we find that there are 584 communities, with an average of 1.2 founders per community. Our model of the founder community on the platform was not accurate.

An interesting finding is that the few most-populous communities are easily recognizable, after which there is a long tail of independent communities with only one or two founders. The three top communities found by LPA are Dorm Room Fund Partners, Dorm Room Fund Portfolio Companies, and YCombinator Portfolio Companies. Given that both of these organizations helped influence this work, it is not surprising to find these communities.

We also ran an alternative connectivity analysis, using the Louvain Method (\textbf{CITE}). This revealed another significant community of founders: Student Founders at UC Berkeley. However, the remainder of the communities still appear to be insignificant.

\subsection{Propensity to Investors}

We sought to answer the question of whether or not the community and neighborhood of a founder's node can predict their propensity to engage with certain investors. However, using graph structure alone, we lack sufficient signal to predict anything. We will later revisit this question, taking into account additional node metrics and founder characteristics.

\section{Predicting Fundraising Success with Graph Metrics}

We would like to explore whether or not linear combinations of simple graph metrics can predict fundraising success. We first will define our metrics, and their intuitive meaning within the context of fundraising.  We hypothesize that the metrics which correspond to commonly-accepted key factors to a foudner's ability to fundraise in venture capital will be highly correlated with our definition of success. We will attempt to validate this hypothesis with our email graph, as well as analyze which factors are indeed the most important.

\subsection{FounderRank}

We wish to score nodes based on graph metrics in an email graph of fundraising relationships. To do this, we first need to define a single scoring function which captures our goals. Thus we introduce FounderRank.

FounderRank is a metric in the range $[0, 1]$ that quickly communicates the strength of a founder in the global fundraising graph of founders, investors, and their mutual connections (i.e. the VCWiz Email Graph). A strong node is able to rapidly spread the word about their startup, start conversations with relevant and desirable investors, and convince investors to invest in them. These abilities are cruical for fundraising, which is in turn crucial for the survival of a startup.

Note that we are currently evaluating how well a founder can fundraise conditioned on them knowing who they would like to fundraise from. We are not tackling the issue of discovering investors, which we have touched on in the previous chapter, and will explore further in the next.

\subsubsection{Core Characteristics}

A combination of existing studies and our own interviews with numerous seed-stage firms reveals three intuitive proprties of a founder's node in a professional or social graph which are desirable when fundraising: \textbf{importance}, \textbf{influence}, and \textbf{access}. Note that these characteristics do not take into account factors such as domain expertise, personality, and pedigree, all of which will also contribute to a successful fundraise. We are focusing exclusively on network properties for this experiment.

The first characteristic is \textbf{importance}. Importance looks at how crucial a founder is in his or her own ecosystem. In efficient professional information networks, we see that ``needs are diagnosed and [the entrepreneur] is passed round the system until [he or she] gathers the necessary information and advice.''~\cite{BIRLEY1985107}. The more crucial a founder is to a domain, the more access to information he or she will have. Thus, founders who have more importance are likely to be seen as a less risky investment, increasing the chances an investor responds positively to a fundraising proposal.

The second characteristic is \textbf{influence}. Influence captures how effectively a founder can effect change in their ecosystem and solicit aid from their peers. In other words, how likely are other founders to help this founder? A study on interorganizational net
works of young companies found supporting evidence to the fact that ``third parties rely on the prominence of the affiliates of those companies to make judgments about their quality''~\cite{10.2307/2666998}. Founders who have high influence can leverage this to convince investors to give them funds.

The third characteristic is \textbf{access}. This is the notion of how well a founder can get in front of the investors of their choice. The more directly connected a founder is to an abritrary investor, the more likely that investor is to respond to an inbound request (either via an introduction or cold) for funding. Additionally, a high degree of access means a founder has many options to chose from when it comes to starting conversations with investors. This follows from the more general finding that proximity in a graph to providers of valuable resources gives a node access to many viable alternatives\cite{10.2307/3069443}. This can be valuable when a founder's top choice of investor does not work out, which is often the case.

\subsubsection{Graph Metrics}

To quantitatively evaluate the impact of each of these characteristics, and measure their predictive capability, we need to find graph metrics which correspond to each.

For \textbf{importance}, we selected PageRank~\cite{page1999pagerank}, the canonical

When it comes to ranking nodes in a graph, there is a large body of literature to reference. The canonical starting point is of course PageRank~\cite{page1999pagerank}, which recursively estimates node importance by analyzing the importance of nodes which link to the node in question.

Conveniently, there are three graph metrics which map directly to these properties.

  1. PageRank (CITE) is a measure of a node's importance in a graph. We use Normalized Page Rank (CITE) to be able to compare across changing graphs.
  2. Betweenness Centrality (CITE) is a measure of a node's influence, based on the number of shortest paths that flow through it.
  3. Closeness Centrality (CITE) is a measure inverse to the sum of a node's distance from every other node in the graph.

To use these three metrics, we first do the same pre-processing described above. We then calculate the raw metric for each node, adjust it as necessary to account for changing graphs, and finally scale the metric to be in the range of [0, 1], based on the global min and max of that metric in the graph. This gives us a number which represents the metric relative to the other nodes in the graph.

The naive version of FounderRank simply averages these three metrics for every node. We also experimented with a weighted version of FounderRank, by attempting to learn the coefficient of each metric based on the successes of the founders in our dataset.

We set out to create a ranking system of founders on the VCWiz platform, based on the data we have collected, in order to evaluate our later work.

\subsection{Evaluation (notes on trying each metric as part of our baseline)}
- Money raised in current round
  - Good when enough time has passed that the amount they raised would have been made public
- Money raised in previous rounds
  - Good indicator of ``repeat founder'', consider making binary
- Intros made
  - Not good, because not enough people used the functionality. It was an experiment. See results.
- number of investors responded
  - looking at the number of unique investors that have exhanged emails with the founder gives a pretty good signal
- \% of investors responded
  - looking at fraction of targets which have emailed back, and fraction of incoming to outgoing is high signal (if greater than one, they are in high demand)
- Average time of response per investor
  - looking at average response time from investors doesn't quite work, as it doesn't take into account phone calls, in-person etc
- Time fundraising (proxy by use of platform? or when email volume went down?)
  - Once founders get to pitch stage, they no longer use the platform, as things are happening in real life. We notice a sharp decline in usage near the end, as it no longer becomes necessary to track that many conversations (PLOT: number of emails vs percentage that are decided). Closing the loop happens automatically with email tracking sometimes, but is far from perfect.
- average sentiment of investor in emails
  - Helps identify founders with strong relations, often corresponding to past exists (PLOT: average sentiment, highest first vs money raised total), but can be noisy

(PLOTs are learnings)

- Best metric, when available, is still aggregate funding of companies the founder has started (including current one)
- Average sentiment of incoming emails from investors is surprsingly high-signal. (PLOT: sentimentvs \# of well-respected investors)
- \# of inbound investors is good, shows a lot of absolute interest
- Email outreach success rates are pretty noisy, as they  are skewed by founders who, for example, send a few emails to already-established connections, or are not the cofounder that sends a lof of emails (but is always cc'ed)
- Number of investors that respond after being added to the founder's wishlists is the strongest signal for companies we have when they are raising

\subsection{Baseline}

Create objective function from all this. Take the index (with ties) of the founder along each metric, and take a weighted average. Note that we are using the weighted average of ranks, not absolute values, so we account for differences in scale.

- Aggregate Funding: 4
- Average Sentiment Inbound: 3
- Number responded after being added to wishlist: 2
- \# of inbounds: 1.5

This gives a nice baseline, with some manual tweaking. Random sampling shows this as a solid baseline that aligns with our priors about venture.

\subsection{Metrics from 2017arXiv170604229H}

Use sum of (Job IPO, Job Acquired, Executive IPO, Executive Acquired, Advisory IPO, and Advisory Acquired). Why? Table 1 \cite{2017arXiv170604229H}. ``We see that the top non-sector features are related to the past experience of the leadership (executive acquisition, executive IPO, advisory IPO, leadership age). The investor feature maximum acquisition fraction is also one of the top non-sector features. This suggests that companies with experienced and successful leadership and investors have increased drift which results in a higher exit probability.''

\subsection{Internal metrics}

- Sentiment, number responded, \# of inbounds ==> indictative of how the fundraise is going, best guess we have. Show some plots that support this.

\subsection{Naive FounderRank}

Now, with naively using the graph metrics we discussed above, how well can we rank these founders? How indicative of founder quality is the founder's email graph? Goal is to rank these so an analyst would only have to triage the top N.

First observation is that using Naive FR without any investment info misses the people we know are good, repeat founders, but may not be actively raising at the moment. Our email graph only goes back 1 year/2 years, and in this time, good founders who can easily get capital will probably just raise based on personal relationships, and put their head down and work. They won't be actively fundraising, which means we won't see as much email activity. This implies that using social graph metrics to track the quality of the founder requires looking back across all history, and/or that it is more useful for new founders when there is not much signal.

- evaluate -> papers

Reference \cite{DBLP:journals/corr/abs-0704-3359}, use the criteria of MRR, DCG, and Precision@n, RMSE. We use metrics of relative regret (such as DCG or P@n), as per \cite{DBLP:journals/corr/abs-0704-3359}, in order to capture the relative importance of the earlier results.

These metrics matter if we want to simply present the best few founders. However, when using FounderRank as a feature for later models, the rankings matter throughout the list, not just at the top. For this, we use metrics such as RMSE or Kendall Tau.

The ranking functions are often used to score a permutation $\pi$ over a document-set, query tuple $(D, q)$. In this case, we assume $q$ is the fixed query of best founder, as defined above.

Loss Functions from \cite{DBLP:journals/corr/abs-0704-3359} (for first goal, of presenting best founders), operating on rank/relevance (== rank in baseline):

WTA and MRR: only make sense when we care about the top document, here we care about the top N that are triagable by a human, say, 10
DCG: takes into account the whole ranking, discounting later ones. Use the numerical rank (largest is best) in the baseline as the relevance. Depends on the labels $y$ (baseline), but these are fixed, so its ok.
Precision@n (\# correct / n): takes into account the number of correct documents in the top N, without taking into account order.

Use DCG, and Precision@n.

Loss functions for using FR as a feature, operating on rank/relevance:

- Rank correlation such as tau and rho, which measure the loss from the ranking implied by the baseline scores
  - tau: measures the agreement (or disagreement) in rank over all pairs
  - rho: measures the direction of association in rank between the two lists

p-test: null hypothesis is that correlation between rankings is 0. p gives probability that two uncorrelated rankings would have this correlation metric value.

Loss functions for using FR as a feature, operating on score:

- Metrics which directly measure the loss of the founder's strength, as determined by the baseline score
  - RMSE (precedent for doing this in \cite{said2014comparative}, ``A common practice with recommender systems is to evaluate their performance through error metrics such as RMSE (root mean squared error)''~\cite{Cremonesi:2010:PRA:1864708.1864721})
  - all the ones in here \cite{Gunawardana:2009:SAE:1577069.1755883}
  - MAE, which gives us the average absolute distance between the score and the ideal score from our baseline

Get these for the baseline. Use rank scaled to 0-1 as the relevance.
Then, treat it as a linear regression problem, with the sole goal of learning the weights on the three metrics, to see which is most important. Write up conclusions from that, compare RMSE of naive and weighted.
Then, see if we can use these three features in other ways, to try to get the best ranking possible, given the labels from the baseline. Use a couple of the fancy methods from \footnote{\url{https://en.wikipedia.org/wiki/Learning_to_rank}}.

Order: baseline, naive, weighted, fancy.
Get [m1, m2, m3, 0-1 output] to use in python for these.

Our naive model has a NDCG of 0.29, and Precision@n of 0, 0.1, and 0.2 for n = 5, 10, and 20 respectively. While these beat out the random model (which is what analysts at VCs are effectively using currently), they don't show a strong case for relying on the absolute rankings generated from the naive model, if the goal is parity. However, if we evaluate our naive model as a feature with the goal of directionally similar rankings to the baseline, we see that the values for tau and rho (0.48 and 0.67 respectively) are more promising. For tau, we see there is reasonable agreement between the rankings, where -1 is rankings that are perfectly opposite, 0 is independent rankings, and 1 is identical rankings. We also see in rho that the direction of association in the rankings generated by the two is positive, and significantly so. This means, more often than not, the ranking of the naive model tends to increase when the baseline does.

This is promising, and it only captures a short timeline of graph interactions, which as we disucssed above, can limit the signal we get for strong founders. Perhaps by increasing the time period over which we are considering emails, we can give stronger graph signal to the top founders, and increase the DCG and Precision@n.

For numbers, look at commit 4489986b.

\subsection{Naive FounderRank (5 years back)}

Let's go back farther and see what happens.

Now up to 558 founders.

Before: NDCG was 0.29.
After: 0.45. Random also up to 0.27, showing that the results after 5 years are more random (expected, as there is more noise)

Precision@n After: Up to 0.2, 0.3, 0.3 for n = 5, 10, 20. Makes sense, as are capturing more of the social graph of repeat founders (who rank higher in the baseline), perhaps from previous raises.

Tau After: Up negligibly. Expected, as this should only influence the top founders, leaving the rest untouched.
Rho: same deal

RMSE: up a notrivial bit to 0.082 from 0.070. expected, because overall more noisy, more random using graph metrics now.
MAE: same deal

\subsection{Naive FounderRank + Frequency}

- Not viable to do in neo4j for now, unclear how high-signal it is.
- Could play around with only considering edges that have at least a certain weight

\subsection{Weighted FounderRank}

If we use the optimal linear combination of all three, we get an $r^2$ of 0.39. Our NDCG is at 0.449, up from 0.448. No change in p@n. Our tau goes to 0.490, and our rho to 0.679. These are small/negligible changes.

Relative to all three:

If we only use pagerank, we get everything worse, except a much higher NDCG (0.527), implying that if we are just worried about surfacing the best founders, they often have high pagerank, and just using pagerank is a good indicator. We get a negative $r^2$, which makes sense given the worse metrics on everything.

If we only use betweenness, everything is worse. We get a negative $r^2$, which makes sense given the worse metrics on everything.

If we only use closeness, we get a lower NDCG (0.375) but a higher P@5 (0.4), but lower P@10 (0.2), with same other P@n. Our tau goes up to 0.498 (marginally) and our RMSE goes up to 0.135. This shows that looking at only closeness (access), we can better predict the ranking of all founders, though we get worse at predicting the exact scores, and we also get worse at surfacing the best founders first. Seems to be the case that access matters more than other metrics in most cases, but a large pagerank or closeness can compensate for this. We get a positive $r^2$ (0.348), which shows that just using closeness goes give us a model that can explain about 35\% the variance of the baseline scores.

PR and closeness: $r^2$ is 0.3916, very close to the optimal weighting of all three. Coefficients are almost the same, with PR being slightly higher (0.423 and 0.389). NDCG actuallty goes up from all three to 0.533, the best out of all of these. Rho and Tau are basically the same.

Since $r^2$ doesn't change much by adding betweenness, it doesn't explain any additional variance, and its highly correlated with the combo of the other two. This is because it is already so highly correlated with PR (0.913), and somewhat correlated to closeness (0.512).

Takeaway: goal is to predict ability to raise based on just graph metrics. emphasis on getting the ``best'' founders right, as lower-ranking founders are more noisy. thus, we evaluate based on Spearman's rho and NDCG. using only closeness (access) gets us almost all the way to the best we can do (rho=0.690, ndcg=0.375), except that is misses some of the top founders that we should be more certain about. Adding in PageRank (importance) solves this, as it is the best at capturing the founders near the top (rho=0.574, ndcg=0.527). Some of the ``best'' founders have such a high PR (such high importance) that it compensates for a lower relative closeness (access). Adding betweenness (influence) to this set of features doesn't change much, because it is highly correlated to PR (importance) (r=0.913), and somewhat correlated to closeness (access) (r=0.512). Ultimately we achieve a Spearman rank correlation (comparing the ranking from just graph metrics to the ``ground-truth'' ranking) of 0.677 with a p-value of 7.28e-77 (effectively 0), and a Normalized Discounted Cumulative Gain of 0.533. This model has an $r^2$ of 0.391, and most of that comes from closeness (adding PR is only 12.8\% increase in $r^2$ == ability to explain variance). Thus we can see that influence is captured by importance, which has an impact on ability to raise, but really it is access which dictates this component of a founder's success.

This conclusion further motivates the work we've done on the VCWiz platform, where one stated goal was to increase access to investors for founders. This conclusion implies that increasing such access would be high-impact, even for those less-well-known founders who have less influence and importance in the ecosystem.

\subsection{FounderRank + Investments}

For founders on the platform, edges implied by a prior investment turn up most of the time in the email logs. For founders off the platform however, we have no access to their email graph. If we use amount raised, metrics from \cite{2017arXiv170604229H} (sum of Job IPO, Job Acquired, Executive IPO, Executive Acquired, Advisory IPO, and Advisory Acquired) as a crude estimate for success, and the same 3 graph metrics, but in the funding graph, as our ranking features, do we see similar results?

Notes:

- Building up ranking graph, we have to add links in both directions, VC to founder and founder to VC, otherwise no PR ever goes to the investors
- We decide to build up the whole social graph of startups. Invested, cofounded, and co-invested. Use this as the second data set to compare to.
- Dataset size: 288,720 founders. 92,843 investors. 57,131 investments.
- Just sorting by number of exits, we see a lot of founders who are now investors. This makes sense, founders who take a company from start to ``finish'' are often what venture firms are looking for in terms of experience.
- Settled on the following baseline, which takes into account that founders who have raised large amounts of capital are more trusted by VCs to take on more money in the future, but that this not nearly as important as the number of times a founder has either lead a company to a successful exit, or participated in such a company (either as an employee or an advisor)

- Affiliated Exits: 4
- Aggregate Funding: 1

Build an investment graph, where nodes are founders and investors, and there's an edge between an investor and founder if the investor funded that founder (individual investors, not firms). See the earlier section for how we inferred which partners at which firms made which investments.

Now same deal. Calculate metrics for whole graph, track it for founders. Look at the same metrics for the naive average, and the weighted version.

1. Sort by baseline metrics
  - Done, with the whole graph. Still need to do only funding graph with backlinks.
2. Run baseline, random
  - Done, looks the same.
3. Run on whole ecosystem graph: naive and weighted
  - Done, gives much less correlation (0.22 tau, 0.33 rho), and this is when we use only closeness. Once again, even more so, looks like closeness is the real important thing.
  - Look at correlation between things. Why is this result much less good? Graph is not representative? Something else going on? Have to sit and think, and also do it on graph from 4.
4. Run on just funded graph: naive and weighted


Now add OUR founders to the platform. We don't have all this historic success data about them, but if we add their email links ot the funding graph, then run everything and rank them, does it align with our original rankings? Can we use this as a proxy with new founders? Simply add their known connections to the graph, run this, and see how they rank?

\section{InvestorRank}

Given that the FounderRank metric seems indicative of a founder's ability to raise, and that raising is a crucial part of keeping a company alive, does ranking investors by the mean founder rank of the founders they've invested in give us a reasonable ranking of investors? Manually inspect it, also compare it to baseline, which can use features like \# of exists, total follow-on raised, and total valuation at IPO sum.

\textbf{TODO}: sort by baseline metrics.

Also see if it makes sense to run these graph metrics for investors on the graph themselves. See is there is correlation between InvestorRank and FounderRank of the founders invested in by that investor. How to do correlation calculation for many to many -- Pairwise?
